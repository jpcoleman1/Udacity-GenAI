{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNf0Bpnbclza3HPIJMujIGk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpcoleman1/Udacity-GenAI/blob/main/udacity_genai_project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ONaeqL4qy8VL"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "from transformers import Trainer\n",
        "from peft import LoraModel, LoraConfig\n",
        "from peft import get_peft_model, TaskType\n",
        "\n",
        "# Import dataset - we are using the ag_news dataset from Huggingface\n",
        "dataset = load_dataset(\"ag_news\", split={'train': 'train', 'test': 'test'})\n",
        "\n",
        "splits = [\"train\", \"test\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare foundation model"
      ],
      "metadata": {
        "id": "df5ThLTusvEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize dataset"
      ],
      "metadata": {
        "id": "zKyhgJOAszfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n"
      ],
      "metadata": {
        "id": "T_wrZX-xMHXP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split dataset"
      ],
      "metadata": {
        "id": "y9W_LPuiAmz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(5000))\n",
        "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
      ],
      "metadata": {
        "id": "m2mkQggpApS-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Pre-trained model"
      ],
      "metadata": {
        "id": "eH4TvAoLs9a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=4,  # AG News has 4 labels\n",
        "    id2label={0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"},\n",
        "    label2id={\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3},\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HT-J0FVMaAh",
        "outputId": "fd89a6ea-3ec3-4a70-ce65-d06aae374b59"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA"
      ],
      "metadata": {
        "id": "kFdLvHrGA_Ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_lin\", \"k_lin\",\"v_lin\"],\n",
        "    lora_dropout=0.01,\n",
        "    task_type=TaskType.SEQ_CLS # Seqence to Classification Task\n",
        ")\n",
        "\n",
        "# Loading the model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=4,\n",
        "    id2label={0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"},\n",
        "    label2id={\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3},\n",
        ")\n"
      ],
      "metadata": {
        "id": "YfvthAmwMk68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8970e579-667b-4074-c915-c52474320bc2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, config)\n",
        "model.config.id2label = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"} # ensure custom lables carry through\n",
        "model.config.label2id = {\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3}\n",
        "\n",
        "\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M11KEOZvGOrq",
        "outputId": "65a3c21c-cf8d-406b-ec0e-5694bfc52552"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 814,852 || all params: 67,771,400 || trainable%: 1.202353795258767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform lightweight tuning"
      ],
      "metadata": {
        "id": "y2KvxMTptH2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training arguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./data/ag_news\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "CK3otfUkMs0a"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define evaluation metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HywVbNVKOHWp",
        "outputId": "d2895114-12fe-4e85-e00d-01ee43402888"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create trainer instance\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,  #\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxr7Wj1wNi_d",
        "outputId": "0253204e-4ad3-4a72-f146-a55fad2e6ebd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "K_mzpyu-tXJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "rXlcrLbwOLdk",
        "outputId": "ed94fb14-277e-4265-e792-118b91637a44"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='626' max='626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [626/626 02:08, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.335462</td>\n",
              "      <td>0.883000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.319200</td>\n",
              "      <td>0.331165</td>\n",
              "      <td>0.883000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 04:54]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory ./data/ag_news/checkpoint-313 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory ./data/ag_news/checkpoint-626 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=626, training_loss=0.31858893933768473, metrics={'train_runtime': 128.5537, 'train_samples_per_second': 77.788, 'train_steps_per_second': 4.87, 'total_flos': 1349753487360000.0, 'train_loss': 0.31858893933768473, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate trained model"
      ],
      "metadata": {
        "id": "o8X249sjtZm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = trainer.evaluate()\n",
        "print(evaluation_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "bfh8yhqwW6_X",
        "outputId": "0bbfb917-d1b5-4ef2-f084-6fc4f97cddc5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 00:04]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.3311645984649658, 'eval_accuracy': 0.883, 'eval_runtime': 4.826, 'eval_samples_per_second': 207.213, 'eval_steps_per_second': 3.315, 'epoch': 2.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model is performing at ~88% accuracy on unseen data. This is a solid performance and likely could be marginally improved with additional training epochs."
      ],
      "metadata": {
        "id": "eb4hajtrtcvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save trained model"
      ],
      "metadata": {
        "id": "UMk-G5bXtn48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./results/ag_news_fine_tuned\")\n",
        "tokenizer.save_pretrained(\"./results/ag_news_fine_tuned\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt01zuCCW95m",
        "outputId": "8e716cca-d50e-4024-a7d9-cc9904815633"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./results/ag_news_fine_tuned/tokenizer_config.json',\n",
              " './results/ag_news_fine_tuned/special_tokens_map.json',\n",
              " './results/ag_news_fine_tuned/vocab.txt',\n",
              " './results/ag_news_fine_tuned/added_tokens.json',\n",
              " './results/ag_news_fine_tuned/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load fine-tuned model (if necessary)"
      ],
      "metadata": {
        "id": "moQkBJK5ts1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "model_path = \"./results/ag_news_fine_tuned\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=4,\n",
        "    id2label={0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"},\n",
        "    label2id={\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3},\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "id": "yjfHJJTnXB4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03b88746-c04f-4fa0-a4df-37c2f3174e05"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "feDhBh34t0m_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, model, tokenizer):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predictions = logits.argmax(-1).tolist()\n",
        "    return [model.config.id2label[prediction] for prediction in predictions]\n",
        "\n",
        "\n",
        "sample_text = \"The stock market closed lower today after a volatile trading session.\"\n",
        "print(predict(sample_text, model, tokenizer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM35CBxhXFLP",
        "outputId": "384b0dce-4ff8-4b5e-9c4b-07eec9e01eb9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Business']\n"
          ]
        }
      ]
    }
  ]
}